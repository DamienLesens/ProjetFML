{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468c4bd8-24cb-4fd7-a6a9-cd81913c20d4",
   "metadata": {},
   "source": [
    "# Attempt with sklearn's MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de25c886-0d4a-4551-9487-a5b81bb85a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e345106b-a59f-4107-92d6-6c18afb12434",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = str(os.getcwd())\n",
    "X_train = np.load(my_path+'/.data/X_train_surge_new.npz')\n",
    "Y_train = np.array(pd.read_csv(my_path+'/.data/Y_train_surge.csv'))\n",
    "X_test = np.load(my_path+'/.data/X_test_surge_new.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f3902a-3f09-462d-977c-7cda601702a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surge_prediction_metric(dataframe_y_true, dataframe_y_pred):\n",
    "    weights = np.linspace(1, 0.1, 10)[np.newaxis]\n",
    "    surge1_columns = [\n",
    "        'surge1_t0', 'surge1_t1', 'surge1_t2', 'surge1_t3', 'surge1_t4',\n",
    "        'surge1_t5', 'surge1_t6', 'surge1_t7', 'surge1_t8', 'surge1_t9' ]\n",
    "    surge2_columns = [\n",
    "        'surge2_t0', 'surge2_t1', 'surge2_t2', 'surge2_t3', 'surge2_t4',\n",
    "        'surge2_t5', 'surge2_t6', 'surge2_t7', 'surge2_t8', 'surge2_t9' ]\n",
    "    surge1_score = (weights * (dataframe_y_true[surge1_columns].values - dataframe_y_pred[surge1_columns].values)**2).mean()\n",
    "    surge2_score = (weights * (dataframe_y_true[surge2_columns].values - dataframe_y_pred[surge2_columns].values)**2).mean()\n",
    "\n",
    "    return surge1_score + surge2_score\n",
    "\n",
    "def metric_one_surge(y_true,y_pred):\n",
    "    weights = np.linspace(1, 0.1, 10)[np.newaxis]\n",
    "    score = (weights*(y_true-y_pred)**2).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d34e9-90a6-4d07-b9c0-2937f7143ab0",
   "metadata": {},
   "source": [
    "### Utilitary functions for ruling 'weights' issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b984644e-5c9b-473b-93ab-d669f335f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be applied to Y_train before learning\n",
    "def transform(y):\n",
    "    weights = np.sqrt(np.linspace(1, 0.1, 10)[np.newaxis])\n",
    "    return weights*y\n",
    "\n",
    "# should be applied to Y_pred after test\n",
    "def inverse_transform(y):\n",
    "    weights = 1/np.sqrt(np.linspace(1, 0.1, 10)[np.newaxis])\n",
    "    return weights*y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74aa7b-2b11-471e-beae-c0bedfb93c6b",
   "metadata": {},
   "source": [
    "## Use MLP only on surge levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0a8d8-2eaa-493e-ba6f-31e9570f8408",
   "metadata": {},
   "source": [
    "**Goal**: Is part surges data enough to predict the tide ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cf26e-4692-4dcd-b97f-b13b4370221b",
   "metadata": {},
   "source": [
    "#### 1. First city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0980ea-9cf5-41db-8bd2-f47051f6bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1st city data\n",
    "surge1_y_train = transform(np.array(Y_train)[:,1:11])\n",
    "\n",
    "surge1_x_train = np.array(X_train['surge1_input'])\n",
    "\n",
    "surge1_x_test = np.array(X_test['surge1_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6373b5e6-16f6-4ebd-9bbd-e597dea51ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='sgd', hidden_layer_sizes=(10,), alpha=1e-5, random_state=1)\n",
    "clf.fit(surge1_x_train, surge1_y_train)\n",
    "surge1_y_test = clf.predict(surge1_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0df9ee3-d6fd-4149-ac5a-383116db5dbe",
   "metadata": {},
   "source": [
    "#### 2. Second city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026ccefc-78af-4600-9745-63f283a0f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2nd city data\n",
    "surge2_y_train = transform(np.array(Y_train)[:,11:])\n",
    "\n",
    "surge2_x_train = np.array(X_train['surge2_input']) \n",
    "\n",
    "surge2_x_test = np.array(X_test['surge2_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ca0de7-4a37-480b-bdb0-5d8a8f840325",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='sgd', hidden_layer_sizes=(10,), alpha=1e-5, random_state=1)\n",
    "clf.fit(surge2_x_train, surge2_y_train)\n",
    "surge2_y_test = clf.predict(surge2_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f274c7-6f98-4929-ae92-062ccbb3e805",
   "metadata": {},
   "source": [
    "Result of this method: 0.88\n",
    "\n",
    "Pretty bad, it shows that we need the information of the slp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0861f-2cde-4fcd-924d-c9c9d9f74c8d",
   "metadata": {},
   "source": [
    "## Take into account slp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e86a6b-4db0-4cf8-960a-1a996adb8e14",
   "metadata": {},
   "source": [
    "### 1. Dimension reduction of slp data (using PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72440d10-ecb0-414e-9abd-d7f7e4738708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slp_to_flat_images(slp):\n",
    "    a,b,c,d = np.shape(slp)\n",
    "    return slp.reshape((a*b,c*d))\n",
    "\n",
    "class Preprocess:\n",
    "    \n",
    "    _pca = PCA()\n",
    "    _scaler1 = StandardScaler()\n",
    "    _scaler2 = StandardScaler()\n",
    "    \n",
    "    def fit_transform(self,slp,nb_comp):\n",
    "        #reshape\n",
    "        list_flat_images = slp_to_flat_images(slp)\n",
    "        print(list_flat_images.shape)\n",
    "        #normalize\n",
    "        list_flat_images = self._scaler1.fit_transform(list_flat_images)\n",
    "        #fit PCA and transform\n",
    "        self._pca = PCA(n_components=nb_comp)\n",
    "        slp_LD = self._pca.fit_transform(list_flat_images)\n",
    "        #reshape\n",
    "        slp_LD = np.array([np.concatenate(slp_LD[i*40:(i+1)*40]) for i in range(len(slp))])\n",
    "        #renormalize\n",
    "        slp_LD = self._scaler2.fit_transform(slp_LD)\n",
    "        return slp_LD\n",
    "    \n",
    "    def transform(self,slp):\n",
    "        flat = slp_to_flat_images(slp)\n",
    "        flat = self._scaler1.transform(flat)\n",
    "        slp_LD = self._pca.transform(flat)\n",
    "        slp_LD = np.array([np.concatenate(slp_LD[i*40:(i+1)*40]) for i in range(len(slp))])\n",
    "        slp_LD = self._scaler2.transform(slp_LD)\n",
    "        print(slp_LD.shape)\n",
    "        print(slp_LD.shape)\n",
    "        return slp_LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ffa5ad-c43a-4f59-b337-73ef7af6fa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(223960, 1681)\n",
      "(5599, 160)\n"
     ]
    }
   ],
   "source": [
    "prepro = Preprocess()\n",
    "slp_train_LD = prepro.fit_transform(X_train['slp'], 4)\n",
    "print(slp_train_LD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf17828-a385-453a-9a7a-18e19e7cd959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(509, 160)\n",
      "(509, 160)\n"
     ]
    }
   ],
   "source": [
    "slp_test_LD = prepro.transform(X_test['slp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597c888",
   "metadata": {},
   "source": [
    "### 2. Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d4387",
   "metadata": {},
   "source": [
    "cross validating for one city at a time as we train different models for both\n",
    "\n",
    "Hyperparameters to test (ordered by priority):\n",
    "- learning rate\n",
    "- activation function\n",
    "- number of dimensions to which we reduce the slp images\n",
    "- hidden layer size (if we follow the rule of thumb this would be 1 layer with (size(input)+size(output))/2 neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730f2fe",
   "metadata": {},
   "source": [
    "Be carefull to do the preprocess after having split the data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c00ad96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.31706080101807926\n",
      "1\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.29046632681123813\n",
      "2\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.31191757439200296\n",
      "3\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.3145051024588383\n",
      "4\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.30736858355615654\n",
      "5\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.2978100972460967\n",
      "6\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.3175804123419718\n",
      "7\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.3161647284684056\n",
      "8\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.30260542371639226\n",
      "9\n",
      "(201560, 1681)\n",
      "(560, 160)\n",
      "(560, 160)\n",
      "error : 0.3083407969217521\n"
     ]
    }
   ],
   "source": [
    "city = 1\n",
    "\n",
    "#possible speedups : flatten slp images before spliting, as we don't learn on that\n",
    "\n",
    "nbs = 10 #number of splits to do the average on\n",
    "test_s = 0.1\n",
    "\n",
    "ss = ShuffleSplit(n_splits=nbs,test_size=test_s)\n",
    "\n",
    "X_ind = np.zeros(5599)\n",
    "\n",
    "for s, (train_index, test_index) in enumerate(ss.split(X_ind)):\n",
    "    print(s)\n",
    "    #split data\n",
    "    slp_train = X_train['slp'][train_index]\n",
    "    slp_test = X_train['slp'][test_index]\n",
    "    \n",
    "    surge_input_train = X_train['surge'+str(city)+'_input'][train_index]\n",
    "    surge_input_test = X_train['surge'+str(city)+'_input'][test_index]\n",
    "    \n",
    "    surge_output_train = Y_train[train_index,1:11] if city==1 else Y_train[train_index,11:]\n",
    "    surge_output_test = Y_train[test_index,1:11] if city==1 else Y_train[test_index,11:]\n",
    "    \n",
    "    #preprocessing\n",
    "    prepro = Preprocess()\n",
    "    slp_train_LD = prepro.fit_transform(slp_train, 4)\n",
    "    slp_test_LD = prepro.transform(slp_test)\n",
    "    \n",
    "    y_train = transform(surge_output_train)\n",
    "    \n",
    "    #concatenate slp and surge input\n",
    "    x_train = np.concatenate((surge_input_train, slp_train_LD), axis=1)\n",
    "    x_test = np.concatenate((surge_input_test, slp_test_LD), axis=1)\n",
    "    \n",
    "    #learn\n",
    "    clf = MLPRegressor(solver='sgd', hidden_layer_sizes=(90,), alpha=1e-5, random_state=1,max_iter=1000)\n",
    "    clf.fit(x_train,transform(y_train))\n",
    "    \n",
    "    #predict\n",
    "    y_pred = inverse_transform(clf.predict(x_test))\n",
    "    \n",
    "    print(\"error :\",metric_one_surge(surge_output_test,y_pred))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71089f9c",
   "metadata": {},
   "source": [
    "Conclusion :\n",
    "\n",
    "Hyperparameters for city 1 :\n",
    "\n",
    "Hyperparameters for city 2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf884c89-b30d-4911-b18c-11a245230727",
   "metadata": {},
   "source": [
    "### 3. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54588b05-5ddb-4b36-892a-5b0a9496bd61",
   "metadata": {},
   "source": [
    "#### 1. First city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40fba6ad-17fe-4a7a-9ccf-72b702e6b757",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 5599 and the array at index 1 has size 5039",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m surge1_y_train \u001b[38;5;241m=\u001b[39m transform(np\u001b[38;5;241m.\u001b[39marray(Y_train)[:,\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m11\u001b[39m])\n\u001b[1;32m      4\u001b[0m surge1_input \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurge1_input\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m surge1_x_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurge1_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslp_train_LD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m surge1_x_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39marray(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurge1_input\u001b[39m\u001b[38;5;124m'\u001b[39m]), slp_test_LD), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(surge1_x_train)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 5599 and the array at index 1 has size 5039"
     ]
    }
   ],
   "source": [
    "# Extract 1st city data\n",
    "surge1_y_train = transform(np.array(Y_train)[:,1:11])\n",
    "\n",
    "surge1_input = np.array(X_train['surge1_input'])\n",
    "surge1_x_train = np.concatenate((surge1_input, slp_train_LD), axis=1)\n",
    "\n",
    "surge1_x_test = np.concatenate((np.array(X_test['surge1_input']), slp_test_LD), axis=1)\n",
    "\n",
    "print(surge1_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca9360-b2e7-4d5d-970f-68e4b6a8b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='sgd', hidden_layer_sizes=(90,), alpha=1e-5, random_state=1,max_iter=1000)\n",
    "clf.fit(surge1_x_train, surge1_y_train)\n",
    "surge1_y_test = clf.predict(surge1_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81276c55-88b9-4177-8001-034ba84d188b",
   "metadata": {},
   "source": [
    "#### 2. Second city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c1c8c-886d-4b7b-8511-00074d9f10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2nd city data\n",
    "surge2_y_train = transform(np.array(Y_train)[:,11:])\n",
    "\n",
    "surge2_input = np.array(X_train['surge2_input'])\n",
    "surge2_x_train = np.concatenate((surge2_input, slp_train_LD), axis=1)\n",
    "\n",
    "surge2_x_test = np.concatenate((np.array(X_test['surge2_input']), slp_test_LD), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438979c8-8444-432a-930f-96c417657d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='sgd', hidden_layer_sizes=(90,), alpha=1e-5, random_state=1,max_iter=1000)\n",
    "clf.fit(surge2_x_train, surge2_y_train)\n",
    "surge2_y_test = clf.predict(surge2_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc84fc-273e-4c0d-bbf1-10eaea52be07",
   "metadata": {},
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80851d2-fc67-4c03-a596-a929022f779d",
   "metadata": {},
   "source": [
    "## Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159efde3-5c3b-4190-bcb9-d2bfdd220798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â transform clf output\n",
    "surge_pred = np.concatenate((inverse_transform(surge1_y_test), inverse_transform(surge2_y_test)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb83014-44d1-499f-b6cd-c80b18e915f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = [f'surge1_t{i}' for i in range(10)] + [f'surge2_t{i}' for i in range(10)]\n",
    "Y_pred = pd.DataFrame(data=surge_pred, columns=y_columns, index=X_test['id_sequence'])\n",
    "Y_pred.to_csv('Y_sep_MLP.csv', index_label='id_sequence', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
