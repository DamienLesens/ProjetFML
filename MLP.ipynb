{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468c4bd8-24cb-4fd7-a6a9-cd81913c20d4",
   "metadata": {},
   "source": [
    "# Attempt with sklearn's MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de25c886-0d4a-4551-9487-a5b81bb85a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.decomposition import PCA\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e345106b-a59f-4107-92d6-6c18afb12434",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = str(os.getcwd())\n",
    "X_train = np.load(my_path+'/.data/X_train_surge_new.npz')\n",
    "Y_train = pd.read_csv(my_path+'/.data/Y_train_surge.csv')\n",
    "X_test = np.load(my_path+'/.data/X_test_surge_new.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f3902a-3f09-462d-977c-7cda601702a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surge_prediction_metric(dataframe_y_true, dataframe_y_pred):\n",
    "    weights = np.linspace(1, 0.1, 10)[np.newaxis]\n",
    "    surge1_columns = [\n",
    "        'surge1_t0', 'surge1_t1', 'surge1_t2', 'surge1_t3', 'surge1_t4',\n",
    "        'surge1_t5', 'surge1_t6', 'surge1_t7', 'surge1_t8', 'surge1_t9' ]\n",
    "    surge2_columns = [\n",
    "        'surge2_t0', 'surge2_t1', 'surge2_t2', 'surge2_t3', 'surge2_t4',\n",
    "        'surge2_t5', 'surge2_t6', 'surge2_t7', 'surge2_t8', 'surge2_t9' ]\n",
    "    surge1_score = (weights * (dataframe_y_true[surge1_columns].values - dataframe_y_pred[surge1_columns].values)**2).mean()\n",
    "    surge2_score = (weights * (dataframe_y_true[surge2_columns].values - dataframe_y_pred[surge2_columns].values)**2).mean()\n",
    "\n",
    "    return surge1_score + surge2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088d34e9-90a6-4d07-b9c0-2937f7143ab0",
   "metadata": {},
   "source": [
    "### Utilitary functions for ruling 'weights' issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b984644e-5c9b-473b-93ab-d669f335f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be applied to Y_train before learning\n",
    "def transform(y):\n",
    "    weights = np.sqrt(np.linspace(1, 0.1, 10)[np.newaxis])\n",
    "    return weights*y\n",
    "\n",
    "# should be applied to Y_pred after test\n",
    "def inverse_transform(y):\n",
    "    weights = 1/np.sqrt(np.linspace(1, 0.1, 10)[np.newaxis])\n",
    "    return weights*y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a74aa7b-2b11-471e-beae-c0bedfb93c6b",
   "metadata": {},
   "source": [
    "## Use MLP only on surge levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0a8d8-2eaa-493e-ba6f-31e9570f8408",
   "metadata": {},
   "source": [
    "**Goal**: Is part surges data enough to predict the tide ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cf26e-4692-4dcd-b97f-b13b4370221b",
   "metadata": {},
   "source": [
    "#### 1. First city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3c0980ea-9cf5-41db-8bd2-f47051f6bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1st city data\n",
    "surge1_y_train = transform(np.array(Y_train)[:,1:11])\n",
    "\n",
    "surge1_input = np.array(X_train['surge1_input'])\n",
    "surge1_x_train = surge1_input\n",
    "\n",
    "surge1_x_test = np.array(X_test['surge1_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6373b5e6-16f6-4ebd-9bbd-e597dea51ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='sgd', hidden_layer_sizes=(10,), alpha=1e-5, random_state=1)\n",
    "clf.fit(surge1_x_train, surge1_y_train)\n",
    "surge1_y_test = clf.predict(surge1_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0df9ee3-d6fd-4149-ac5a-383116db5dbe",
   "metadata": {},
   "source": [
    "#### 2. Second city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "026ccefc-78af-4600-9745-63f283a0f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2nd city data\n",
    "surge2_y_train = transform(np.array(Y_train)[:,11:])\n",
    "\n",
    "surge2_input = np.array(X_train['surge2_input'])\n",
    "surge2_x_train = surge1_input\n",
    "\n",
    "surge2_x_test = np.array(X_test['surge2_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1ca0de7-4a37-480b-bdb0-5d8a8f840325",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='sgd', hidden_layer_sizes=(10,), alpha=1e-5, random_state=1)\n",
    "clf.fit(surge2_x_train, surge2_y_train)\n",
    "surge2_y_test = clf.predict(surge2_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f274c7-6f98-4929-ae92-062ccbb3e805",
   "metadata": {},
   "source": [
    "Result of this method: 0.88\n",
    "\n",
    "Pretty bad, it shows that we need the information of the slp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0861f-2cde-4fcd-924d-c9c9d9f74c8d",
   "metadata": {},
   "source": [
    "## Take into account slp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e86a6b-4db0-4cf8-960a-1a996adb8e14",
   "metadata": {},
   "source": [
    "### 1. Dimension reduction of slp data (using PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72440d10-ecb0-414e-9abd-d7f7e4738708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slp_to_flat_images(slp, scaler):\n",
    "    l = []\n",
    "    for x in slp:\n",
    "        for i in x:\n",
    "            l.append(scaler.transform(i.flatten()))\n",
    "    return np.array(l)\n",
    "\n",
    "def transform_pca_slp(pca,slp, scaler):\n",
    "    flat = slp_to_flat_images(slp, scaler)\n",
    "    slp_LD = pca.transform(flat)\n",
    "    slp_LD_reshape = np.array([np.concatenate(slp_LD[i*40:(i+1)*40]) for i in range(len(slp))])\n",
    "    print(slp_LD.shape)\n",
    "    print(slp_LD_reshape.shape)\n",
    "    return slp_LD\n",
    "\n",
    "def fit_transform_pca_slp(slp,nb_comp):\n",
    "    scaler = StandardScaler()\n",
    "    s = np.shape(slp)\n",
    "    scaler.fit(slp.reshape((s[0],s[1]*s[2]*s[3])))\n",
    "    #flatten\n",
    "    list_flat_images = slp_to_flat_images(slp, scaler)\n",
    "    #fit PCA\n",
    "    pca = PCA(n_components=nb_comp)\n",
    "    pca.fit(list_flat_images)\n",
    "    #transform\n",
    "    slp_LD = transform_pca_slp(pca,slp)\n",
    "    \n",
    "    return pca,slp_LD,scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6ffa5ad-c43a-4f59-b337-73ef7af6fa71",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pca, slp_train_LD, scaler \u001b[38;5;241m=\u001b[39m fit_transform_pca_slp(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslp\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "pca, slp_train_LD, scaler = fit_transform_pca_slp(X_train['slp'], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf17828-a385-453a-9a7a-18e19e7cd959",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m slp_test_LD \u001b[38;5;241m=\u001b[39m transform_pca_slp(\u001b[43mpca\u001b[49m, X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pca' is not defined"
     ]
    }
   ],
   "source": [
    "slp_test_LD = transform_pca_slp(pca, X_test['slp'], scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf884c89-b30d-4911-b18c-11a245230727",
   "metadata": {},
   "source": [
    "### 2. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54588b05-5ddb-4b36-892a-5b0a9496bd61",
   "metadata": {},
   "source": [
    "#### 1. First city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40fba6ad-17fe-4a7a-9ccf-72b702e6b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1st city data\n",
    "surge1_y_train = transform(np.array(Y_train)[:,1:11])\n",
    "\n",
    "surge1_input = np.array(X_train['surge1_input'])\n",
    "surge1_x_train = np.concatenate((surge1_input, slp_train_LD), axis=1)\n",
    "\n",
    "surge1_x_test = np.concatenate((np.array(X_test['surge1_input']), slp_test_LD), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e8ca9360-b2e7-4d5d-970f-68e4b6a8b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='sgd', hidden_layer_sizes=(10,), alpha=1e-5, random_state=1)\n",
    "clf.fit(surge1_x_train, surge1_y_train)\n",
    "surge1_y_test = clf.predict(surge1_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81276c55-88b9-4177-8001-034ba84d188b",
   "metadata": {},
   "source": [
    "#### 2. Second city:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "476c1c8c-886d-4b7b-8511-00074d9f10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2nd city data\n",
    "surge2_y_train = transform(np.array(Y_train)[:,11:])\n",
    "\n",
    "surge2_input = np.array(X_train['surge2_input'])\n",
    "surge2_x_train = np.concatenate((surge2_input, slp_train_LD), axis=1)\n",
    "\n",
    "surge2_x_test = np.concatenate((np.array(X_test['surge2_input']), slp_test_LD), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "438979c8-8444-432a-930f-96c417657d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='sgd', hidden_layer_sizes=(10,), alpha=1e-5, random_state=1)\n",
    "clf.fit(surge2_x_train, surge2_y_train)\n",
    "surge2_y_test = clf.predict(surge2_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc84fc-273e-4c0d-bbf1-10eaea52be07",
   "metadata": {},
   "source": [
    "Result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80851d2-fc67-4c03-a596-a929022f779d",
   "metadata": {},
   "source": [
    "## Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "159efde3-5c3b-4190-bcb9-d2bfdd220798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform clf output\n",
    "surge_pred = np.concatenate((inverse_transform(surge1_y_test), inverse_transform(surge2_y_test)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1bb83014-44d1-499f-b6cd-c80b18e915f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = [f'surge1_t{i}' for i in range(10)] + [f'surge2_t{i}' for i in range(10)]\n",
    "Y_pred = pd.DataFrame(data=surge_pred, columns=y_columns, index=X_test['id_sequence'])\n",
    "Y_pred.to_csv('Y_sep_MLP.csv', index_label='id_sequence', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
